memo: defalut
# --- Defaults ---
hid_size: 64 #hidden layer size
total_epoches: 10000 #number of training epochs
epoch_size: 20 #number of update iterations in an epoch = How many processes to run
use_multiprocessing: False #use multiprocessing to parallelize the process
use_offline_wandb: False # Log results to wandb
grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm
use_cuda: False # Use GPU


# --- Environment ---
# auto update, you don't need to change this
n_agents: 1 #Number of agents (used in multiagent)
n_actions: 1 #Number of actions
obs_shape: 32 #Observation size
state_shape: 32 #State size = observation size * n_agents



# --- pymarl options ---



buffer_cpu_only: True # If true we won't keep all of the replay buffer in vram
max_steps: 20 #force to end the game after this many steps
normalize_rewards: False #normalize rewards in each batch
action_scale: 1.0 #scale action output from model
random: False #enable random model


# --- Logging options ---
use_tensorboard: False # Log results to tensorboard

save: False # Save the models to disk
save_every: 50000 # save the model after every n_th epoch
#checkpoint_path: "" # Load a checkpoint from this path
#evaluate: False # Evaluate model for test_nepisode episodes and quit (no training)
#load_step: 0 # Load model trained on this many timesteps (0 if choose max possible)
#save_replay: False # Saving the replay of the model loaded from checkpoint_path
local_results_path: "results" # Path for local results

# --- RL hyperparameters ---
batch_size: 500 # number of steps before each update (per thread)
lr: 0.001 #learning rate
tau: 1.0
gamma: 1.0 #discount factor
lamda: 0.95 #gae factor
qk_hid_size: 16 #key and query size for soft attention
value_hid_size: 32 #value size for soft attention

entr: 0 #entropy regularization coeff
value_coeff: 0.01 #coeff for value loss term

add_value_last_step: True
recurrent: False #make the model recurrent in time


# ---CommNet specific args---
commnet: False #enable commnet model
ic3net: False #enable ic3net model
tarcomm: False #enable tarmac model (with commnet or ic3net)
gacomm: False #enable gacomm model
magic: False
#nagents: 1 #Number of agents (used in multiagent)
comm_mode: 'avg' #Type of mode for communication tensor calculation [avg|sum]
comm_passes: 1 #Number of comm passes per step over the model
comm_mask_zero: False #Whether communication should be there
mean_ratio: 1.0 #how much coooperative to do? 1.0 means fully cooperative
rnn_type: 'MLP' #type of rnn to use. [LSTM|MLP]
detach_gap: 10000 #detach hidden state and cell state for rnns at this interval.' + ' Default 10000 (very high)
comm_init: 'uniform' #how to initialise comm weights [uniform|zeros]
hard_attn: False #Whether to use hard attention: action - talk|silent
comm_action_one: False #Whether to always talk, sanity check for hard attention.
advantages_per_action: False #Whether to multipy log porb for each chosen action with advantages
share_weights: False #Share weights for hops