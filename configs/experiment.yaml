memo: defalut
total_epoches: 10000 #number of training epochs
epoch_size: 2 #number of update iterations in an epoch = How many processes to run
use_multiprocessing: False #use multiprocessing to parallelize the process
use_offline_wandb: False # Log results to wandb
grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm
use_cuda: False # Use GPU
normalize_rewards: False
gamma: 1.0
lr: 0.001 #learning rate
value_coeff: 0.01 #coeff for value loss term in the loss function
batch_size: 500
n_processes: 1 #number of processes to use for training